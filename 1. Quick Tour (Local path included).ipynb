{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhangkhare/Transformers/blob/main/1.%20Quick%20Tour%20(Local%20path%20included).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XRl4dFrWDZhG",
        "outputId": "6f97dc1b-9ce7-4e32-f7a3-019ecdf26157",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets -q\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtAoWVB3DZhI"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zsVoXuHDZhI"
      },
      "source": [
        "[pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) is the easiest way to use a pretrained model for a given task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "hide_input": true,
        "id": "VMxecMKPDZhI",
        "outputId": "f3a3a143-ae20-4a7a-827c-c870a9e722b9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJfFHhnODZhI"
      },
      "source": [
        "The [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) supports many common tasks out-of-the-box:\n",
        "\n",
        "**Text**:\n",
        "* Sentiment analysis: classify the polarity of a given text.\n",
        "* Text generation (in English): generate text from a given input.\n",
        "* Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.).\n",
        "* Question answering: extract the answer from the context, given some context and a question.\n",
        "* Fill-mask: fill in the blank given a text with masked words.\n",
        "* Summarization: generate a summary of a long sequence of text or document.\n",
        "* Translation: translate text into another language.\n",
        "* Feature extraction: create a tensor representation of the text.\n",
        "\n",
        "**Image**:\n",
        "* Image classification: classify an image.\n",
        "* Image segmentation: classify every pixel in an image.\n",
        "* Object detection: detect objects within an image.\n",
        "\n",
        "**Audio**:\n",
        "* Audio classification: assign a label to a given segment of audio.\n",
        "* Automatic speech recognition (ASR): transcribe audio data into text.\n",
        "\n",
        "<Tip>\n",
        "\n",
        "For more details about the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) and associated tasks, refer to the documentation [here](https://huggingface.co/docs/transformers/main/en/./main_classes/pipelines).\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Local usage of pipeline - Sentiment Analysis"
      ],
      "metadata": {
        "id": "pbsRpcO9HQ9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive"
      ],
      "metadata": {
        "id": "FLvoJUNjQMBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IFyR-hgREg7I",
        "outputId": "f6282d4e-e1c3-4209-addc-34f75d427dff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd /content/drive/MyDrive/transformers/"
      ],
      "metadata": {
        "id": "zBugxSB0urJI",
        "outputId": "334f95d2-a5eb-4ad2-9ccd-8a314c611ad4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone 'https://github.com/shubhangkhare/Transformers'"
      ],
      "metadata": {
        "id": "lDXYJm02u99O",
        "outputId": "0fdd2206-62c7-4a3d-c3d9-fb2084730d0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Transformers'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 21 (delta 10), reused 0 (delta 0), pack-reused 0\n",
            "Unpacking objects: 100% (21/21), 73.43 KiB | 228.00 KiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and save model"
      ],
      "metadata": {
        "id": "UdASCZLXQQL4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSxPocbcDZhJ",
        "outputId": "609bf756-c910-4c2f-fb3b-40c3254d554f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier.save_pretrained(\"/content/drive/MyDrive/transformers/distilbert-base-uncased-finetuned-sst-2-english/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier.save_pretrained(\"/content/drive/MyDrive/transformers/Transformers/distilbert-base-uncased-finetuned-sst-2-english/\")'''"
      ],
      "metadata": {
        "id": "RDacjYLwvcAS",
        "outputId": "2bddc9ed-8e06-41e7-f080-a0c2880c513d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load pipeline from local path"
      ],
      "metadata": {
        "id": "2bb1eib8RPMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", model = \"/content/drive/MyDrive/transformers/distilbert-base-uncased-finetuned-sst-2-english/\")"
      ],
      "metadata": {
        "id": "cLStWwFPGsoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pHA9cVCDZhJ"
      },
      "source": [
        "The pipeline downloads and caches a default [pretrained model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) and tokenizer for sentiment analysis. Now you can use the `classifier` on your target text:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference pipeline"
      ],
      "metadata": {
        "id": "2_czcTmyRSm5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk678sL7DZhJ",
        "outputId": "3b623afb-d572-4135-9f68-ba2265b6d3f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "classifier(\"We are very happy to show you the 🤗 Transformers library.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMcV9KszDZhJ"
      },
      "source": [
        "For more than one sentence, pass a list of sentences to the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) which returns a list of dictionaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnzK6uSJDZhJ",
        "outputId": "c1f59e9f-c067-46ee-d210-654b49c2a98f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label: POSITIVE, with score: 0.9998\n",
              "label: NEGATIVE, with score: 0.5309"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = classifier([\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfMzdD8ODZhJ"
      },
      "source": [
        "The [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) can also iterate over an entire dataset. Start by installing the [🤗 Datasets](https://huggingface.co/docs/datasets/) library:\n",
        "\n",
        "```bash\n",
        "pip install datasets\n",
        "```\n",
        "\n",
        "Create a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) with the task you want to solve for and the model you want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7x-MlbaDZhJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIul21LMDZhJ"
      },
      "source": [
        "Next, load a dataset (see the 🤗 Datasets [Quick Start](https://huggingface.co/docs/datasets/quickstart.html) for more details) you'd like to iterate over. For example, let's load the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JTb1hkSDZhK"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Audio\n",
        "\n",
        "dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HmrfA1ADZhK"
      },
      "source": [
        "We need to make sure that the sampling rate of the dataset matches the sampling\n",
        "rate `facebook/wav2vec2-base-960h` was trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBUcrSMNDZhK"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TmN3lPvDZhK"
      },
      "source": [
        "Audio files are automatically loaded and resampled when calling the `\"audio\"` column.\n",
        "Let's extract the raw waveform arrays of the first 4 samples and pass it as a list to the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCKXKhIlDZhK",
        "outputId": "b233cbaf-d853-4b4e-ffc2-49790d782d38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', \"FONDERING HOW I'D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE\", \"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS\", 'HOW DO I TURN A JOIN A COUNT']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = speech_recognizer(dataset[:4][\"audio\"])\n",
        "print([d[\"text\"] for d in result])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjYbRBLVDZhK"
      },
      "source": [
        "For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the [pipeline documentation](https://huggingface.co/docs/transformers/main/en/./main_classes/pipelines) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di9gCnqADZhK"
      },
      "source": [
        "# Use another model and tokenizer in the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and save model"
      ],
      "metadata": {
        "id": "qdg75TNjSYgF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDD57pn-DZhK"
      },
      "source": [
        "The [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) can accommodate any model from the [Model Hub](https://huggingface.co/models), making it easy to adapt the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) for other use-cases. For example, if you'd like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual [BERT model](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) fine-tuned for sentiment analysis. Great, let's use this model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jg-9dGARDZhK"
      },
      "outputs": [],
      "source": [
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LDgujUjDZhK"
      },
      "source": [
        "Use the [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification) and [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer) to load the pretrained model and it's associated tokenizer (more on an `AutoClass` below):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdEUq491DZhK",
        "outputId": "250d2f2a-4fa1-4509-c207-efa0511ba17c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/transformers/bert-base-multilingual-uncased-sentiment/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/transformers/bert-base-multilingual-uncased-sentiment/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/transformers/bert-base-multilingual-uncased-sentiment/vocab.txt',\n",
              " '/content/drive/MyDrive/transformers/bert-base-multilingual-uncased-sentiment/added_tokens.json',\n",
              " '/content/drive/MyDrive/transformers/bert-base-multilingual-uncased-sentiment/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "model.save_pretrained('/content/drive/MyDrive/transformers/bert-base-multilingual-uncased-sentiment/')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/transformers/bert-base-multilingual-uncased-sentiment/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model from local path"
      ],
      "metadata": {
        "id": "rpoY5P1hS9E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_name = '/content/drive/MyDrive/transformers/bert-base-multilingual-uncased-sentiment/'\n",
        "model = AutoModelForSequenceClassification.from_pretrained(new_model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(new_model_name)"
      ],
      "metadata": {
        "id": "hmecNDpVTCC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-GTWp8NDZhK"
      },
      "source": [
        "Use the [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification) and [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer) to load the pretrained model and it's associated tokenizer (more on an `TFAutoClass` below):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTdw-GB6DZhK"
      },
      "outputs": [],
      "source": [
        "'''from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(new_model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(new_model_name)''';"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96-LP6SsDZhK"
      },
      "source": [
        "Then you can specify the model and tokenizer in the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline), and apply the `classifier` on your target text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfEy3ANdDZhK",
        "outputId": "ddcd140b-d103-4c78-b16a-6cda69364add",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': '5 stars', 'score': 0.7272651791572571}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "classifier(\"Nous sommes très heureux de vous présenter la bibliothèque 🤗 Transformers.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEY3_V4LDZhL"
      },
      "source": [
        "If you can't find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our [fine-tuning tutorial](https://huggingface.co/docs/transformers/main/en/./training) to learn how. Finally, after you've fine-tuned your pretrained model, please consider sharing it (see tutorial [here](https://huggingface.co/docs/transformers/main/en/./model_sharing)) with the community on the Model Hub to democratize NLP for everyone! 🤗"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hNGD4ftDZhL"
      },
      "source": [
        "## AutoClass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "hide_input": true,
        "id": "iOns7W58DZhL",
        "outputId": "cbd0cd97-e966-45f9-a42a-ab0f737b2b3f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AhChOFRegn4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AhChOFRegn4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajdMTCw-DZhL"
      },
      "source": [
        "Under the hood, the [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification) and [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer) classes work together to power the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). An [AutoClass](https://huggingface.co/docs/transformers/main/en/./model_doc/auto) is a shortcut that automatically retrieves the architecture of a pretrained model from it's name or path. You only need to select the appropriate `AutoClass` for your task and it's associated tokenizer with [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer).\n",
        "\n",
        "Let's return to our example and see how you can use the `AutoClass` to replicate the results of the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKxWEGVYDZhL"
      },
      "source": [
        "### AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feitzq05DZhL"
      },
      "source": [
        "A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called *tokens*. There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization [here](https://huggingface.co/docs/transformers/main/en/./tokenizer_summary)). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you're using the same tokenization rules a model was pretrained with.\n",
        "\n",
        "Load a tokenizer with [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJh-8TURDZhL"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "new_model_name = '/content/drive/MyDrive/transformers/bert-base-multilingual-uncased-sentiment/'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(new_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nal5ssJoDZhM"
      },
      "source": [
        "Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model's *vocabulary*.\n",
        "\n",
        "Pass your text to the tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0vK0hvsDZhM",
        "outputId": "036817ee-39b4-4a31-e299-0b46e2b22475",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer(\"We are very happy to show you the 🤗 Transformers library.\")\n",
        "print(encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nMOcW1IDZhM"
      },
      "source": [
        "The tokenizer will return a dictionary containing:\n",
        "\n",
        "* [input_ids](https://huggingface.co/docs/transformers/main/en/./glossary#input-ids): numerical representions of your tokens.\n",
        "* [atttention_mask](https://huggingface.co/docs/transformers/main/en/.glossary#attention-mask): indicates which tokens should be attended to.\n",
        "\n",
        "Just like the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline), the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyHHW-2sDZhM"
      },
      "outputs": [],
      "source": [
        "pt_batch = tokenizer(\n",
        "    [\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crr-x6HJDZhM"
      },
      "outputs": [],
      "source": [
        "tf_batch = tokenizer(\n",
        "    [\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"tf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJnauQgRDZhM"
      },
      "source": [
        "Read the [preprocessing](https://huggingface.co/docs/transformers/main/en/./preprocessing) tutorial for more details about tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARdxgKMVDZhM"
      },
      "source": [
        "### AutoModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1ekcN_VDZhM"
      },
      "source": [
        "🤗 Transformers provides a simple and unified way to load pretrained instances. This means you can load an [AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel) like you would load an [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer). The only difference is selecting the correct [AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel) for the task. Since you are doing text - or sequence - classification, load [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWV-cwfRDZhM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "pt_model = AutoModelForSequenceClassification.from_pretrained(new_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx-UajjNDZhM"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "See the [task summary](https://huggingface.co/docs/transformers/main/en/./task_summary) for which [AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel) class to use for which task.\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding `**`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAqQ85zlDZhM"
      },
      "outputs": [],
      "source": [
        "pt_outputs = pt_model(**pt_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm74tOxdDZhM"
      },
      "source": [
        "The model outputs the final activations in the `logits` attribute. Apply the softmax function to the `logits` to retrieve the probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5TWmZPmDZhM",
        "outputId": "fcd28992-7304-47b5-be21-23b3902bab50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n",
            "        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
        "print(pt_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8GRhyZaDZhM"
      },
      "source": [
        "🤗 Transformers provides a simple and unified way to load pretrained instances. This means you can load an [TFAutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModel) like you would load an [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer). The only difference is selecting the correct [TFAutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModel) for the task. Since you are doing text - or sequence - classification, load [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh9-Nzd0DZhN",
        "outputId": "70b35040-7189-40dc-e4ab-f6b361bec3bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "#tf_model = TFAutoModelForSequenceClassification.save_pretrained(new_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tf_model = TFAutoModelForSequenceClassification.from_pretrained(new_model_name)"
      ],
      "metadata": {
        "id": "gfbsRY3RfRAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ2F4vTyDZhN"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "See the [task summary](https://huggingface.co/docs/transformers/main/en/./task_summary) for which [AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel) class to use for which task.\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Now you can pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RDqAMBWDZhN"
      },
      "outputs": [],
      "source": [
        "tf_outputs = tf_model(tf_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLgKJGN4DZhN"
      },
      "source": [
        "The model outputs the final activations in the `logits` attribute. Apply the softmax function to the `logits` to retrieve the probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ529rK7DZhN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n",
        "tf_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uImMFc97DZhN"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "All 🤗 Transformers models (PyTorch or TensorFlow) outputs the tensors *before* the final activation\n",
        "function (like softmax) because the final activation function is often fused with the loss.\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Models are a standard [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) or a [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) so you can use them in your usual training loop. However, to make things easier, 🤗 Transformers provides a [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the `fit` method from [Keras](https://keras.io/). Refer to the [training tutorial](https://huggingface.co/docs/transformers/main/en/./training) for more details.\n",
        "\n",
        "<Tip>\n",
        "\n",
        "🤗 Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.\n",
        "The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `None` are ignored.\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J0XUDSSDZhN"
      },
      "source": [
        "### Save a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5mL3YtRDZhO"
      },
      "source": [
        "Once your model is fine-tuned, you can save it with its tokenizer using [PreTrainedModel.save_pretrained()](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzkQy6SXDZhO"
      },
      "outputs": [],
      "source": [
        "pt_save_directory = \"./pt_save_pretrained\"\n",
        "tokenizer.save_pretrained(pt_save_directory)\n",
        "pt_model.save_pretrained(pt_save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKQwDQCsDZhO"
      },
      "source": [
        "When you are ready to use the model again, reload it with [PreTrainedModel.from_pretrained()](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXmNWgVeDZhO"
      },
      "outputs": [],
      "source": [
        "pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g40m_WzxDZhO"
      },
      "source": [
        "Once your model is fine-tuned, you can save it with its tokenizer using [TFPreTrainedModel.save_pretrained()](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3ayoe04DZhO"
      },
      "outputs": [],
      "source": [
        "tf_save_directory = \"./tf_save_pretrained\"\n",
        "tokenizer.save_pretrained(tf_save_directory)\n",
        "tf_model.save_pretrained(tf_save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7Yyt8ADDZhO"
      },
      "source": [
        "When you are ready to use the model again, reload it with [TFPreTrainedModel.from_pretrained()](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQIpWdtkDZhO"
      },
      "outputs": [],
      "source": [
        "tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0peAJkz6DZhO"
      },
      "source": [
        "One particularly cool 🤗 Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The `from_pt` or `from_tf` parameter can convert the model from one framework to the other:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czWWRoZ3DZhO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n",
        "pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX7pUSmuDZhO"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
        "tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}